{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNbagfCYEuI2quUmstE2Rwb",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jsmnlao/Google-2B-Search-Query-Recommendation-System/blob/main/Final_Search_Query_Model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "####################### IMPORT NECESSARY LIBRARIES #######################\n",
        "%pip install --upgrade tiktoken -q #download tiktoken\n",
        "%pip install --upgrade openai -q\n",
        "\n",
        "import nltk\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tiktoken\n",
        "import torch\n",
        "import os\n",
        "\n",
        "nltk.download('punkt')\n",
        "\n",
        "####################### MOVE THE TRAINING TO GPU USING .DEVICE #######################\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "####################### LOAD OUR DATASET AND SPLIT TO TRAIN AND VALIDATION #######################\n",
        "splits = {'train': 'nq_open/train-00000-of-00001.parquet', 'validation': 'nq_open/validation-00000-of-00001.parquet'}\n",
        "dft = pd.read_parquet(\"hf://datasets/google-research-datasets/nq_open/\" + splits[\"train\"])\n",
        "dfv = pd.read_parquet(\"hf://datasets/google-research-datasets/nq_open/\" + splits[\"validation\"])\n",
        "df = pd.concat([dft, dfv])\n",
        "\n",
        "####################### FEATURE ENGINEERING #######################\n",
        "df = df.drop(columns=['answer'])\n",
        "# print(df.head(10))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VwqTsk2c26vm",
        "outputId": "bdfcfa69-2e54-4cd7-d34a-0e66ec28dd8c"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m389.5/389.5 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "####################### TIKTOKEN ENCODING FOR FIRST LEVEL OF ENCODING #######################\n",
        "enc = tiktoken.get_encoding(\"cl100k_base\") #loading encoding\n",
        "\n",
        "DELIMITER = \"|\"\n",
        "\n",
        "# This only keeps the first 2k records to make iteration on the model\n",
        "# faster. The full training data set will need to be used for real runs of the model.\n",
        "training_blob = \"|\".join(df['question'].to_list()[:2000])    # Only uses the first 2000 records for now for faster iterations, NEED TO CHANGE LATER\n",
        "# training_blob = \"|\".join(df['question'].to_list())\n",
        "TRAINING_SIZE = len(training_blob)\n",
        "print(\"TRAINING_SIZE: \", TRAINING_SIZE)\n",
        "\n",
        "training_blob_encoded = enc.encode(training_blob)\n",
        "\n",
        "unique_tokens = set(training_blob_encoded)\n",
        "\n",
        "####################### COMPRESS NUMBER OF TOKENS USED FOR SECOND LEVEL OF ENCODING #######################\n",
        "ordnial_to_token = {i: v for i, v in enumerate(sorted(unique_tokens))}\n",
        "token_to_ordinal = {v: i for i, v in enumerate(sorted(unique_tokens))}\n",
        "\n",
        "def encode_ticktokens(ticktoken_tokens: list[int]) -> list[int]:\n",
        "  return [token_to_ordinal[t] for t in ticktoken_tokens]\n",
        "\n",
        "def decode_to_ticktokens(ordinals: list[int]) -> list[int]:\n",
        "  return [ordnial_to_token[t] for t in ordinals]\n",
        "\n",
        "training_blob_double_encoded = encode_ticktokens(training_blob_encoded)\n",
        "\n",
        "training_data_tensor = torch.tensor(training_blob_double_encoded, dtype=torch.long)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zEdKa8l_3PNs",
        "outputId": "528cda1f-68bb-4510-fa02-4c16c1d673e8"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TRAINING_SIZE:  96322\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_aj6uORj2uaF",
        "outputId": "a8fb3df2-c6e5-4609-e09f-77867e197051"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Length of test_data, training data:  2262 20366\n",
            "VOCAB SIZE:  4273\n"
          ]
        }
      ],
      "source": [
        "####################### PERFORM HOLDOUT WITH 10% #######################\n",
        "holdout_size = int(len(training_data_tensor) * .1)\n",
        "holdout_size\n",
        "\n",
        "test_data = training_data_tensor[:holdout_size]\n",
        "training_data = training_data_tensor[holdout_size:]\n",
        "print(\"Length of test_data, training data: \", len(test_data), len(training_data))\n",
        "\n",
        "BATCH_SIZE = 4\n",
        "BLOCK_SIZE = 8\n",
        "\n",
        "####################### FUNCTION THAT PREPARES BATCHES OF THE DATASET #######################\n",
        "def get_batch(split):\n",
        "  data = training_data if split == 'train' else test_data\n",
        "  ix = torch.randint(len(data) - BLOCK_SIZE, (BATCH_SIZE,))\n",
        "\n",
        "  x = torch.stack([data[i:i+BLOCK_SIZE] for i in ix])\n",
        "  y = torch.stack([data[i+1:i+BLOCK_SIZE+1] for i in ix])\n",
        "  return x, y\n",
        "\n",
        "# print(get_batch('train'))\n",
        "\n",
        "VOCAB_SIZE = len(set(training_blob_double_encoded))\n",
        "print(\"VOCAB SIZE: \", VOCAB_SIZE)\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "import random\n",
        "\n",
        "VOCAB_SIZE = len(set(training_blob_double_encoded))\n",
        "NUM_EMBEDDINGS = VOCAB_SIZE // 2\n",
        "\n",
        "# @torch.no_grad()\n",
        "# def estimate_loss():\n",
        "#   out = {}\n",
        "#   model.eval()\n",
        "#   for split in ['train', 'val']:\n",
        "#     losses = torch.zeros(eval_iters)\n",
        "#     for k in range(eval_iters):\n",
        "#       X, Y = get_batch(split)\n",
        "#       logits, loss = model(X, Y)\n",
        "#       losses[k] = loss.item()\n",
        "#     out[split] = losses.mean()\n",
        "#   model.train()\n",
        "#   return out\n",
        "\n",
        "####################### HEAD COMPONENT #######################\n",
        "class Head(nn.Module):\n",
        "  def __init__(self, head_size):\n",
        "    super().__init__()\n",
        "    self.key = nn.Linear(NUM_EMBEDDINGS, head_size, bias=False)\n",
        "    self.query = nn.Linear(NUM_EMBEDDINGS, head_size, bias=False)\n",
        "    self.value = nn.Linear(NUM_EMBEDDINGS, head_size, bias=False)\n",
        "    self.register_buffer('tril', torch.tril(torch.ones(BLOCK_SIZE, BLOCK_SIZE)))\n",
        "\n",
        "  def forward(self, x):\n",
        "    B,T,C = x.shape\n",
        "    k = self.key(x)\n",
        "    q = self.query(x)\n",
        "    wei = q @ k.transpose(-2, -1) * C**-0.5\n",
        "    wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf'))\n",
        "    wei = F.softmax(wei, dim=-1)\n",
        "    v = self.value(x)\n",
        "    out = wei @ v\n",
        "    return out\n",
        "\n",
        "####################### MULTI-HEAD ATTENTION COMPONENT #######################\n",
        "class MultiHeadAttention(nn.Module):\n",
        "  def __init__(self, num_heads, head_size):\n",
        "    super().__init__()\n",
        "    self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
        "    self.proj = nn.Linear(NUM_EMBEDDINGS, NUM_EMBEDDINGS)\n",
        "\n",
        "  def forward(self, x):\n",
        "    out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
        "    out = self.proj(out)\n",
        "    return out\n",
        "\n",
        "####################### FEED FORWARD COMPONENT #######################\n",
        "class FeedForward(nn.Module):\n",
        "  def __init__(self, n_embed):\n",
        "    super().__init__()\n",
        "    self.net = nn.Sequential(\n",
        "        nn.Linear(n_embed, 4*n_embed),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(4*n_embed, n_embed),\n",
        "    )\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.net(x)\n",
        "\n",
        "####################### TRANSFORMER BLOCK USING MULTI-HEAD ATTENTION, FEED FORWARD NETWORK, AND LAYER NORMALIZATION #######################\n",
        "class Block(nn.Module):\n",
        "  def __init__(self, n_embed, n_head):\n",
        "    super().__init__()\n",
        "    head_size = n_embed // n_head\n",
        "    self.sa = MultiHeadAttention(n_head, head_size)\n",
        "    self.ffwd = FeedForward(n_embed)\n",
        "    self.ln1 = nn.LayerNorm(n_embed)\n",
        "    self.ln2 = nn.LayerNorm(n_embed)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = x + self.sa(self.ln1(x))\n",
        "    x = x + self.ffwd(self.ln2(x))\n",
        "    return x\n",
        "\n",
        "####################### ACTUAL TRANSFORMER MODEL #######################\n",
        "class BigramLanguageModel(nn.Module):\n",
        "\n",
        "  def __init__(self, vocab_size):\n",
        "    super().__init__()\n",
        "    self.token_embedding_table = nn.Embedding(vocab_size, NUM_EMBEDDINGS)\n",
        "    self.position_embedding_table = nn.Embedding(BLOCK_SIZE, NUM_EMBEDDINGS)\n",
        "    #self.sa_head = Head(NUM_EMBEDDINGS)\n",
        "    self.feed_forward = FeedForward(NUM_EMBEDDINGS)\n",
        "    self.sa_heads = MultiHeadAttention(4, NUM_EMBEDDINGS//4)\n",
        "    self.lm_head = nn.Linear(NUM_EMBEDDINGS, vocab_size)\n",
        "    self.blocks = nn.Sequential(\n",
        "      Block(NUM_EMBEDDINGS, n_head=4),\n",
        "      Block(NUM_EMBEDDINGS, n_head=4),\n",
        "      Block(NUM_EMBEDDINGS, n_head=4),\n",
        "      nn.LayerNorm(NUM_EMBEDDINGS),\n",
        "    )\n",
        "    self.proj = nn.Linear(NUM_EMBEDDINGS, NUM_EMBEDDINGS)\n",
        "\n",
        "  ####################### FORWARD FUNCTION FOR TRAINING AND EVALUATION #######################\n",
        "  def forward(self, token, targets=None):\n",
        "    token_embeddings = self.token_embedding_table(token)\n",
        "    B, T = token.shape\n",
        "    position_embeddings = self.position_embedding_table(torch.arange(T))\n",
        "    x = token_embeddings + position_embeddings\n",
        "    x = self.blocks(x)\n",
        "\n",
        "    logits = self.lm_head(x)\n",
        "\n",
        "    if targets is None:\n",
        "      loss = None\n",
        "    else:\n",
        "      B, T, C = logits.shape\n",
        "      logits = logits.view(B*T, C)\n",
        "      targets = targets.view(B*T)\n",
        "      loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "    return logits, loss\n",
        "\n",
        "#   def generate(self, idx, max_new_tokens):\n",
        "#     for _ in range(max_new_tokens):\n",
        "#       logits, _ = self(idx)\n",
        "#       logits = logits[:, -1, :] # (B, C)\n",
        "#       probs = F.softmax(logits, dim=-1) # (B, C)\n",
        "#       idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
        "#       idx = torch.cat((idx, idx_next), dim=1)\n",
        "#     return idx\n",
        "\n",
        "  ####################### GENERATE FUNCTION FOR GENERATING SEQUENCES GIVEN TOKENS #######################\n",
        "  def generate(self, idx, max_new_tokens):\n",
        "    for _ in range(max_new_tokens):\n",
        "      idx_cond = idx[:, -BLOCK_SIZE:]\n",
        "      logits, loss = self(idx_cond)\n",
        "      logits = logits[:, -1, :] # (B, C)\n",
        "      probs = F.softmax(logits, dim=-1) # (B, C)\n",
        "      idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
        "      idx = torch.cat((idx, idx_next), dim=1)\n",
        "    return idx\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if os.path.exists('./saved_bigram_language_model_v2.pth'):\n",
        "  print(\"v2 exists\")\n",
        "if os.path.exists('./saved_bigram_language_model.pth'):\n",
        "  print(\"v1 exists\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UXPYHy3Y4YgD",
        "outputId": "c664ef54-c3d4-4332-d2fc-c570d46acae1"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "v2 exists\n",
            "v1 exists\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "LEARNING_RATE = 1e-3\n",
        "MAX_ITERS = 10000\n",
        "EVAL_INTERVAL = 100\n",
        "EVAL_ITERS = 100\n",
        "\n",
        "####################### FUNCTION THAT EVALUATES THE MODEL PERFORMANCE ON TRAINING AND VALIDATION #######################\n",
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "  out = {}\n",
        "  m.eval()\n",
        "  for split in ['train', 'val']:\n",
        "    losses = torch.zeros(EVAL_ITERS)\n",
        "    for k in range(EVAL_ITERS):\n",
        "      X, Y = get_batch(split)\n",
        "      logits, loss = m(X, Y)\n",
        "      losses[k] = loss.item()\n",
        "    out[split] = losses.mean()\n",
        "  m.train()\n",
        "  return out\n",
        "\n",
        "####################### INTIALIZE THE MODEL #######################\n",
        "m = BigramLanguageModel(VOCAB_SIZE)\n",
        "optimizer = torch.optim.Adam(m.parameters(), lr=1e-3)\n",
        "saved_model_path = './saved_bigram_language_model_v2.pth'\n",
        "\n",
        "# IF SAVED MODEL FILE EXISTS: Restore the model with the saved parameters and weights\n",
        "if os.path.exists(saved_model_path):\n",
        "  print(f\"Loading saved model parameters from {saved_model_path}...\")\n",
        "  m.load_state_dict(torch.load(saved_model_path, weights_only=True))\n",
        "  m.eval()\n",
        "\n",
        "# ELSE: Start training model and then save to file path\n",
        "else:\n",
        "  print(f\"{saved_model_path} was not found. Starting the training loop from scratch...\")\n",
        "\n",
        "  ####################### TRAINING LOOP FOR TRANSFORMER MODEL #######################\n",
        "  for iter in range(MAX_ITERS):\n",
        "\n",
        "    if iter % EVAL_INTERVAL == 0:\n",
        "      losses = estimate_loss()\n",
        "      print(f\"iter {iter}; train loss {losses['train']:.4f}; val loss {losses['val']:.4f}\")\n",
        "\n",
        "    xb, yb = get_batch('train')\n",
        "\n",
        "    logits, loss = m(xb, yb)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "  print(f\"Final Loss: {loss.item()}\")\n",
        "  torch.save(m.state_dict(), saved_model_path)\n",
        "  print(f\"Model was saved to {saved_model_path}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hBukCQ_x3Yxd",
        "outputId": "ebf02519-4e9c-4650-cd16-b26868b3a02e"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading saved model parameters from ./saved_bigram_language_model_v2.pth...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "####################### EVALUATE THE MODEL ON AN EXAMPLE INPUT #######################\n",
        "print(\"...Beginning to evaluate the model on example input...\")\n",
        "\n",
        "input = \"where did they film\"\n",
        "print(\"Example input: \", input)\n",
        "\n",
        "input_encoded = enc.encode(input)\n",
        "print(\"First level of encoded input: \", input_encoded)\n",
        "\n",
        "input_double_encoded = [encode_ticktokens(input_encoded)]\n",
        "print(\"Second level of encoded input: \", input_double_encoded)\n",
        "\n",
        "example_token_tensor = torch.tensor(input_double_encoded, dtype=torch.long)\n",
        "print(\"Tensor of encoded input: \", example_token_tensor)\n",
        "\n",
        "print(enc.decode(decode_to_ticktokens(m.generate(example_token_tensor, 40)[0].tolist())).split('|')[0])\n",
        "\n",
        "print(\"#-----------------------------------------------------------------------------------#\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hydXi0pK6bcf",
        "outputId": "b0ec1b91-025e-4685-a8b2-d77ca6df31be"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "...Beginning to evaluate the model on example input...\n",
            "Example input:  where did they film\n",
            "First level of encoded input:  [2940, 1550, 814, 4632]\n",
            "Second level of encoded input:  [[862, 538, 310, 1187]]\n",
            "Tensor of encoded input:  tensor([[ 862,  538,  310, 1187]])\n",
            "where did they film for hero start\n",
            "#-----------------------------------------------------------------------------------#\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input2 = \"how do you make\"\n",
        "print(\"Example input: \", input2)\n",
        "\n",
        "input2_encoded = enc.encode(input2)\n",
        "print(\"First level of encoded input: \", input2_encoded)\n",
        "\n",
        "input2_double_encoded = [encode_ticktokens(input2_encoded)]\n",
        "print(\"Second level of encoded input: \", input2_double_encoded)\n",
        "\n",
        "example2_token_tensor = torch.tensor(input2_double_encoded, dtype=torch.long)\n",
        "print(\"Tensor of encoded input: \", example2_token_tensor)\n",
        "\n",
        "print(enc.decode(decode_to_ticktokens(m.generate(example2_token_tensor, 40)[0].tolist())).split('|')[0])\n",
        "\n",
        "print(\"#-----------------------------------------------------------------------------------#\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v4iCqSVU6b1C",
        "outputId": "5eee9127-8fd8-47d1-c42d-5dd96c816f5d"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Example input:  how do you make\n",
            "First level of encoded input:  [5269, 656, 499, 1304]\n",
            "Second level of encoded input:  [[1288, 251, 174, 468]]\n",
            "Tensor of encoded input:  tensor([[1288,  251,  174,  468]])\n",
            "how do you make we first center of thier\n",
            "#-----------------------------------------------------------------------------------#\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input3 = \"who is the current\"\n",
        "print(\"Example input: \", input3)\n",
        "\n",
        "input3_encoded = enc.encode(input3)\n",
        "print(\"First level of encoded input: \", input3_encoded)\n",
        "\n",
        "input3_double_encoded = [encode_ticktokens(input3_encoded)]\n",
        "print(\"Second level of encoded input: \", input3_double_encoded)\n",
        "\n",
        "example3_token_tensor = torch.tensor(input3_double_encoded, dtype=torch.long)\n",
        "print(\"Tensor of encoded input: \", example3_token_tensor)\n",
        "\n",
        "print(enc.decode(decode_to_ticktokens(m.generate(example3_token_tensor, 10)[0].tolist())).split('|')[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OXQaOdyy5KVi",
        "outputId": "a2e602b5-22c6-44f0-e3d8-7a85872079d4"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Example input:  who is the current\n",
            "First level of encoded input:  [14965, 374, 279, 1510]\n",
            "Second level of encoded input:  [[2366, 118, 58, 522]]\n",
            "Tensor of encoded input:  tensor([[2366,  118,   58,  522]])\n",
            "who is the current king of the moon\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wvvvH_5L6BHT"
      },
      "execution_count": 13,
      "outputs": []
    }
  ]
}